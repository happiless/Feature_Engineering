### 机器学习流程
##### 

### 2. 数值
##### 2.1 标量、向量和空间
    单个数字特征也称为标量。标量的有序列表称为向量。向量位于向量空间中。
##### 2.2 处理计数
    二值化
    二进制化
    量化或装箱
        固定宽度装箱
        分位数装箱
        
##### 2.3 对数变换
    boxcox变换
    
##### 2.4 特征缩放 / 归一化
    Min-max缩放
    标准化（方差缩放）
    不要中心化稀疏数据
        L2 normalization
    数据空间与特征空间
##### 2.5 交互特征
    组合特征
    x1, x2 ---> x1,x1*x1, x1*x2, x2*x2, x2
##### 2.6 特征选择
    特征选择技术会删除非有用的特征，以降低最终模型的复杂性。
    粗略地说, 特征选择技术分为三类
        Filtering（过滤）: 
            预处理可以删除那些不太可能对模型有用的特征, 最好先保守地进行预过滤，以免在进行模型训练步骤之前无意中消除有用的特征
        Wrapper methods（包装方法）: 
            包装方法将模型视为提供特征子集质量分数的黑盒子。是一个独立的方法迭代地改进子集
        Embedded methods（嵌入式方法）: 
            决策树固有地执行特征选择
            𝐿1 鼓励模型使用一些特征而不是许多特征
            
            不如包装方法那么强大，但也远不如包装方法那么昂贵。
            与过滤相比，嵌入式方法会选择特定于模型的特征。

### 3. 文本数据: 扁平化、过滤和分块
    
##### 3.1 元素袋: 将自然文本转换为扁平向量
##### 3.2 使用过滤获取清洁特征
##### 3.3 意义的单位: 从单词、n元词到短语

### 4. 特征缩放的效果: 从词袋到tf-idf
##### 4.1 tf-idf: 词袋的一种简单扩展
##### 4.2 tf-idf方法测试
##### 4.3 深入研究: 发生了什么

### 5. 分类变量: 自动化时代的数据计数
##### 5.1 分类变量的编码
    one-hot编码
    dummy编码: 将one-hot编码的的任意一个状态去掉
        虚拟编码和独热编码都是在Pandas中以pandas.get_dummies的形式实现的
    Effect编码
        分类变量编码的另一种变体称为Effect编码
    优缺点:
         独热编码是多余的， 但优点是每个特征都明显对应于一个类别，失踪数据可以编码为全零矢量，输出应该是整体目标变量的平均值
         虚拟编码和效果编码不是多余的， 他们产生独特和可解释的模型

##### 5.2 处理大型分类变量
    1. 对编码不做任何事情。 使用便宜的训练简单模型。 在许多机器上将独热编码引入线性模型（逻辑回归或线性支持向量机）
    2. 压缩编码，有两种方式
        a. 对特征进行哈希  --在线性回归中特别常见
        b. bin-counting   --在线性回归中与树模型都常见
    
    特征哈希: hash碰撞
    
    bin-counting
    
    关于稀有类别
    
##### 5.3 总结
    one-hot encoding
        空间复杂度： 𝑂(𝑛) 
        时间复杂度： 𝑂(𝑛𝑘)
        优点:
            容易实现
            更高的精度
            在线学习特别容易扩展
        缺点:
            计算不足
            如果类别增加则不能够使用
            对线性模型以外的任何其他方法都不可行
            对于大数据集需要分布式训练
    Feature hashing
        空间复杂度： 𝑂(𝑛) 
        时间复杂度： 𝑂(𝑛𝑚)
        优点:
            容易实现
            容易训练
            容易扩展到新类别
            容易处理稀有类别
            在线学习容易扩展
        缺点:
            只能够使用线性或核模型
            哈希编码很难解释
            精度有争议
    Bin-counting
        空间复杂度： 𝑂(𝑛+𝑘) 
        时间复杂度： 𝑂(𝑛)
        优点:
            训练快
            能够使用树模型
            容易扩展到新列类别
            容易处理稀有类别
            可解释
        缺点：
            需要利用历史信息
            对于在线学习有困难
            会有数据泄露
    
    线性模型比较简单，因此可以进行训练处理非压缩表示，例如独热编码。 

    基于树的模型，另一方面，需要反复搜索右侧分割的所有特征，并且是因此限于小型表示，如箱计数。 
    
    哈希函数处于在这两个极端之间，但是由此产生的精确度各有不同。

### 6. 数据降维: 使用PCA挤压数据
##### 6.1 直观理解
##### 6.2 数学推导
##### 6.3 PCA实战
##### 6.4 白化于ZCA
##### 6.5 PCA的局限性与注意事项
##### 6.6 用例

### 7. 非线性特征与 k-均值模型堆叠
##### 7.1 k-均值聚类
##### 7.2 使用聚类进行曲面拼接
##### 7.3 用于分类问题的k-均值特征化
##### 7.4 优点、缺点以及陷阱

### 8. 自动特征生成: 图像特征提取和深度学习
##### 8.1 最简单的图像特征 (以及它们因何失效)
##### 8.2 人工特征提取 (SIFT 和 HOG)
##### 8.3 通过深度神经网络学习图像特征

### 9. 回到特征: 建立学术论文推荐器
##### 9.1 基于项目的协同过滤
##### 9.2 第一关: 数据导入、清洗和特征解析
##### 9.3 第二关: 更多特征工程和更智能的模型
##### 9.4 第三关: 更多特征 = 更多信息
